{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN-HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CdO_O8ws5wPh",
        "HJpVmTbPlUOS",
        "AW_-4dPPlZf1"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOAw0Jh4ognj5yoWufCYgHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AUT-Student/NN-HW3/blob/main/NN_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "slLXpefU8jnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "j5_bZGnS8izB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "f2F3uD7W8KMH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xwyeeLyzCMF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0c6d73-dc82-4e14-ae61-9ce502fd2915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\n",
            "To: /content/Dataset.zip\n",
            "100% 61.0M/61.0M [00:00<00:00, 101MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip -O \"/content/Dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDM9CJlO8vzQ",
        "outputId": "1fd6dd6b-8752-4781-d8a8-e72a4620973d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Dataset.zip\n",
            "   creating: UCI HAR Dataset/\n",
            "  inflating: UCI HAR Dataset/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/UCI HAR Dataset/\n",
            "  inflating: __MACOSX/UCI HAR Dataset/._.DS_Store  \n",
            "  inflating: UCI HAR Dataset/activity_labels.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._activity_labels.txt  \n",
            "  inflating: UCI HAR Dataset/features.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._features.txt  \n",
            "  inflating: UCI HAR Dataset/features_info.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._features_info.txt  \n",
            "  inflating: UCI HAR Dataset/README.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._README.txt  \n",
            "   creating: UCI HAR Dataset/test/\n",
            "   creating: UCI HAR Dataset/test/Inertial Signals/\n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_x_test.txt  \n",
            "   creating: __MACOSX/UCI HAR Dataset/test/\n",
            "   creating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/\n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_x_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_y_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_acc_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_acc_z_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_x_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_x_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_y_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/body_gyro_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._body_gyro_z_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_x_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_x_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_y_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/Inertial Signals/total_acc_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/Inertial Signals/._total_acc_z_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._Inertial Signals  \n",
            "  inflating: UCI HAR Dataset/test/subject_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._subject_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/X_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._X_test.txt  \n",
            "  inflating: UCI HAR Dataset/test/y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/test/._y_test.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._test  \n",
            "   creating: UCI HAR Dataset/train/\n",
            "   creating: UCI HAR Dataset/train/Inertial Signals/\n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_x_train.txt  \n",
            "   creating: __MACOSX/UCI HAR Dataset/train/\n",
            "   creating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/\n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_x_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_y_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_acc_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_acc_z_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_x_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_y_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._body_gyro_z_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_x_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_x_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_y_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/Inertial Signals/total_acc_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/Inertial Signals/._total_acc_z_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._Inertial Signals  \n",
            "  inflating: UCI HAR Dataset/train/subject_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._subject_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/X_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._X_train.txt  \n",
            "  inflating: UCI HAR Dataset/train/y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/train/._y_train.txt  \n",
            "  inflating: __MACOSX/UCI HAR Dataset/._train  \n",
            "  inflating: __MACOSX/._UCI HAR Dataset  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names -O \"/content/Dataset.names\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7js37rWe8tck",
        "outputId": "40487a00-492a-48d8-c053-b33bfff3862e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.names\n",
            "To: /content/Dataset.names\n",
            "\r  0% 0.00/6.30k [00:00<?, ?B/s]\r100% 6.30k/6.30k [00:00<00:00, 4.29MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"/content/Dataset.names\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oINwMqh094xN",
        "outputId": "87b11259-2cf3-434a-abdb-64f6b1fdeb29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================================================================\r\n",
            "Human Activity Recognition Using Smartphones Dataset\r\n",
            "Version 1.0\r\n",
            "===================================================================================================\r\n",
            "Jorge L. Reyes-Ortiz(1,2), Davide Anguita(1), Alessandro Ghio(1), Luca Oneto(1) and Xavier Parra(2)\r\n",
            "1 - Smartlab - Non-Linear Complex Systems Laboratory\r\n",
            "DITEN - Universit�  degli Studi di Genova, Genoa (I-16145), Italy. \r\n",
            "2 - CETpD - Technical Research Centre for Dependency Care and Autonomous Living\r\n",
            "Universitat Polit�cnica de Catalunya (BarcelonaTech). Vilanova i la Geltr� (08800), Spain\r\n",
            "activityrecognition '@' smartlab.ws \r\n",
            "===================================================================================================\r\n",
            "\r\n",
            "The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. \r\n",
            "\r\n",
            "The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details. \r\n",
            "\r\n",
            "For each record it is provided:\r\n",
            "======================================\r\n",
            "\r\n",
            "- Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\r\n",
            "- Triaxial Angular velocity from the gyroscope. \r\n",
            "- A 561-feature vector with time and frequency domain variables. \r\n",
            "- Its activity label. \r\n",
            "- An identifier of the subject who carried out the experiment.\r\n",
            "\r\n",
            "The dataset includes the following files:\r\n",
            "=========================================\r\n",
            "\r\n",
            "- 'README.txt'\r\n",
            "\r\n",
            "- 'features_info.txt': Shows information about the variables used on the feature vector.\r\n",
            "\r\n",
            "- 'features.txt': List of all features.\r\n",
            "\r\n",
            "- 'activity_labels.txt': Links the class labels with their activity name.\r\n",
            "\r\n",
            "- 'train/X_train.txt': Training set.\r\n",
            "\r\n",
            "- 'train/y_train.txt': Training labels.\r\n",
            "\r\n",
            "- 'test/X_test.txt': Test set.\r\n",
            "\r\n",
            "- 'test/y_test.txt': Test labels.\r\n",
            "\r\n",
            "The following files are available for the train and test data. Their descriptions are equivalent. \r\n",
            "\r\n",
            "- 'train/subject_train.txt': Each row identifies the subject who performed the activity for each window sample. Its range is from 1 to 30. \r\n",
            "\r\n",
            "- 'train/Inertial Signals/total_acc_x_train.txt': The acceleration signal from the smartphone accelerometer X axis in standard gravity units 'g'. Every row shows a 128 element vector. The same description applies for the 'total_acc_x_train.txt' and 'total_acc_z_train.txt' files for the Y and Z axis. \r\n",
            "\r\n",
            "- 'train/Inertial Signals/body_acc_x_train.txt': The body acceleration signal obtained by subtracting the gravity from the total acceleration. \r\n",
            "\r\n",
            "- 'train/Inertial Signals/body_gyro_x_train.txt': The angular velocity vector measured by the gyroscope for each window sample. The units are radians/second. \r\n",
            "\r\n",
            "Notes: \r\n",
            "======\r\n",
            "- Features are normalized and bounded within [-1,1].\r\n",
            "- Each feature vector is a row on the text file.\r\n",
            "- The units used for the accelerations (total and body) are 'g's (gravity of earth -> 9.80665 m/seg2).\r\n",
            "- The gyroscope units are rad/seg.\r\n",
            "- A video of the experiment including an example of the 6 recorded activities with one of the participants can be seen in the following link: http://www.youtube.com/watch?v=XOEN9W05_4A\r\n",
            "\r\n",
            "For more information about this dataset please contact: activityrecognition '@' smartlab.ws\r\n",
            "\r\n",
            "License:\r\n",
            "========\r\n",
            "Use of this dataset in publications must be acknowledged by referencing the following publication [1] \r\n",
            "\r\n",
            "[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \r\n",
            "\r\n",
            "This dataset is distributed AS-IS and no responsibility implied or explicit can be addressed to the authors or their institutions for its use or misuse. Any commercial use is prohibited.\r\n",
            "\r\n",
            "Other Related Publications:\r\n",
            "===========================\r\n",
            "[2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L. Reyes-Ortiz.  Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living: Home Care.   Volume 19, Issue 9. May 2013\r\n",
            "\r\n",
            "[3] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living, IWAAL 2012, Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture Notes in Computer Science 2012, pp 216-223. \r\n",
            "\r\n",
            "[4] Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas, Davide Anguita, Joan Cabestany, Andreu Catal�. Human Activity and Motion Disorder Recognition: Towards Smarter Interactive Cognitive Environments. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013.  \r\n",
            "\r\n",
            "==================================================================================================\r\n",
            "Jorge L. Reyes-Ortiz, Alessandro Ghio, Luca Oneto, Davide Anguita and Xavier Parra. November 2013.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat \"/content/UCI HAR Dataset/activity_labels.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlHR2zgU-Yk3",
        "outputId": "aa253e47-b701-40b9-ae05-77e7d008ce8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 WALKING\n",
            "2 WALKING_UPSTAIRS\n",
            "3 WALKING_DOWNSTAIRS\n",
            "4 SITTING\n",
            "5 STANDING\n",
            "6 LAYING\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv(\"/content/UCI HAR Dataset/train/X_train.txt\", delimiter=r\"\\s+\", header=None)\n",
        "X_test = pd.read_csv(\"/content/UCI HAR Dataset/test/X_test.txt\", delimiter=r\"\\s+\", header=None)\n",
        "y_train = pd.read_csv(\"/content/UCI HAR Dataset/train/y_train.txt\", delimiter=r\"\\s+\", header=None)\n",
        "y_test = pd.read_csv(\"/content/UCI HAR Dataset/test/y_test.txt\", delimiter=r\"\\s+\", header=None)"
      ],
      "metadata": {
        "id": "V3JPy5L5qgHW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[\"Class\"] = y_train\n",
        "X_test[\"Class\"] = y_test"
      ],
      "metadata": {
        "id": "ehah1xhWFG1E"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid, X_train = np.split(X_train.sample(frac=1, random_state=0), [int(.125*len(X_train))])"
      ],
      "metadata": {
        "id": "eFqW_pxw_2Yk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = X_train[\"Class\"]\n",
        "X_train = X_train.drop(columns=[\"Class\"])\n",
        "\n",
        "y_valid = X_valid[\"Class\"]\n",
        "X_valid = X_valid.drop(columns=[\"Class\"])"
      ],
      "metadata": {
        "id": "pGez1UEkH119"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.apply(lambda x:x-1)\n",
        "y_test = y_test.apply(lambda x:x-1)\n",
        "y_valid = y_valid.apply(lambda x:x-1)"
      ],
      "metadata": {
        "id": "6pUkZ-WvQxoB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Classifer"
      ],
      "metadata": {
        "id": "CdO_O8ws5wPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(keras.Model):\n",
        "  def __init__(self, number_hidden_layers, number_hidden_units):\n",
        "    super().__init__()\n",
        "\n",
        "    if number_hidden_layers<0:\n",
        "      raise Exception(\"The number of hidden layers must be a non-negetive number\")\n",
        "\n",
        "    if number_hidden_layers != len(number_hidden_units):\n",
        "      raise Exception(\"The number of hidden layers must equal to the length of the number of hidden units list\")\n",
        "\n",
        "    self.model = keras.models.Sequential()\n",
        "\n",
        "    for i in range(number_hidden_layers):\n",
        "      self.model.add(keras.layers.Dense(units=number_hidden_units[i], activation=\"relu\", name=f\"Dense_Layer_{i+1}\"))\n",
        "\n",
        "    self.model.add(keras.layers.Dense(6, activation=\"softmax\", name=\"Output_Layer\"))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return self.model(inputs)"
      ],
      "metadata": {
        "id": "MrD12o6V2KpU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(number_hidden_layers, number_hidden_units, learning_rate):\n",
        "  es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", restore_best_weights=True, patience=5)\n",
        "\n",
        "  model = MLPModel(number_hidden_layers=number_hidden_layers, number_hidden_units=number_hidden_units)\n",
        "\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "  \n",
        "  model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  model.fit(x=X_train, y=y_train, epochs=30, validation_data=(X_valid, y_valid), verbose=0, callbacks=[es_callback])\n",
        "\n",
        "  train_accuracy = model.evaluate(X_train, y_train, verbose=0)[1]\n",
        "  validation_accuracy = model.evaluate(X_valid, y_valid, verbose=0)[1]\n",
        "\n",
        "  print(f\"Train = {round(train_accuracy*100, 2)}, Validation = {round(validation_accuracy*100, 2)}\")"
      ],
      "metadata": {
        "id": "FEgCM9BW7kat"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Fine-tuning"
      ],
      "metadata": {
        "id": "HJpVmTbPlUOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for learning_rate in [0.0001, 0.0005, 0.001, 0.003, 0.005, 0.01, 0.1]:\n",
        "  print(f\"Learning Rate = {learning_rate}\")\n",
        "  train_and_evaluate(number_hidden_layers=3, number_hidden_units=[128, 64, 32], learning_rate=0.001)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5OGkKLNifjj",
        "outputId": "a8f46aad-d80b-45df-e932-f1f6dacd3e41"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate = 0.0001\n",
            "Train = 98.83, Validation = 97.71\n",
            "\n",
            "Learning Rate = 0.0005\n",
            "Train = 98.85, Validation = 97.82\n",
            "\n",
            "Learning Rate = 0.001\n",
            "Train = 97.95, Validation = 96.52\n",
            "\n",
            "Learning Rate = 0.003\n",
            "Train = 98.8, Validation = 98.04\n",
            "\n",
            "Learning Rate = 0.005\n",
            "Train = 98.4, Validation = 97.17\n",
            "\n",
            "Learning Rate = 0.01\n",
            "Train = 98.77, Validation = 97.93\n",
            "\n",
            "Learning Rate = 0.1\n",
            "Train = 98.57, Validation = 97.93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number Layer and Number Neuron Fine-tuning"
      ],
      "metadata": {
        "id": "AW_-4dPPlZf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_evaluate(number_hidden_layers=0, number_hidden_units=[], learning_rate=0.003)\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNEc3An_i0uz",
        "outputId": "aadfe69a-f222-46cf-da5f-64a3b0a0af13"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train = 98.82, Validation = 97.93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for number_hidden_units in [[32], [64], [128]]:\n",
        "  print(f\"number_hidden_units = {number_hidden_units}\")\n",
        "  train_and_evaluate(number_hidden_layers=1, number_hidden_units=number_hidden_units, learning_rate=0.003)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySIXtRVvlR01",
        "outputId": "3a002eb4-411c-4dec-b507-2f4b440859a1"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_hidden_units = [32]\n",
            "Train = 98.96, Validation = 97.93\n",
            "\n",
            "number_hidden_units = [64]\n",
            "Train = 98.87, Validation = 97.82\n",
            "\n",
            "number_hidden_units = [128]\n",
            "Train = 98.87, Validation = 98.04\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for number_hidden_units in [\n",
        "                            [32, 32], [64, 32], [128, 128]\n",
        "                            ]:\n",
        "  print(f\"number_hidden_units = {number_hidden_units}\")\n",
        "  train_and_evaluate(number_hidden_layers=2, number_hidden_units=number_hidden_units, learning_rate=0.003)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoDRMe3-l79A",
        "outputId": "17de5942-3702-42da-c709-e5c88edd04ad"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_hidden_units = [32, 32]\n",
            "Train = 98.87, Validation = 98.26\n",
            "\n",
            "number_hidden_units = [64, 32]\n",
            "Train = 98.4, Validation = 97.06\n",
            "\n",
            "number_hidden_units = [128, 128]\n",
            "Train = 97.28, Validation = 96.63\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for number_hidden_units in [\n",
        "                            [64, 32, 32], [64, 64, 64], [128, 64, 32]\n",
        "                            ]:\n",
        "  print(f\"number_hidden_units = {number_hidden_units}\")\n",
        "  train_and_evaluate(number_hidden_layers=3, number_hidden_units=number_hidden_units, learning_rate=0.003)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg4yKaXemQJu",
        "outputId": "90586100-816b-435d-b9bb-ef90f3136c95"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_hidden_units = [64, 32, 32]\n",
            "Train = 98.49, Validation = 97.71\n",
            "\n",
            "number_hidden_units = [64, 64, 64]\n",
            "Train = 98.15, Validation = 97.39\n",
            "\n",
            "number_hidden_units = [128, 64, 32]\n",
            "Train = 98.26, Validation = 97.28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for number_hidden_units in [\n",
        "                            [128, 128, 64, 64], [128, 64, 32, 16], [128, 64, 32, 64]\n",
        "                            ]:\n",
        "  print(f\"number_hidden_units = {number_hidden_units}\")\n",
        "  train_and_evaluate(number_hidden_layers=4, number_hidden_units=number_hidden_units, learning_rate=0.05)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cRFD9AFmaDH",
        "outputId": "347173c5-6496-488e-d248-97b970f34093"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number_hidden_units = [128, 128, 64, 64]\n",
            "Train = 18.76, Validation = 18.17\n",
            "\n",
            "number_hidden_units = [128, 64, 32, 16]\n",
            "Train = 52.45, Validation = 50.82\n",
            "\n",
            "number_hidden_units = [128, 64, 32, 64]\n",
            "Train = 18.76, Validation = 18.17\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOM"
      ],
      "metadata": {
        "id": "19ALn6EJrSdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colorsys\n",
        "\n",
        "class SOM():\n",
        "  def __init__(self, number_epochs, map_size, number_features,\n",
        "               initial_learning_rate, initial_radius, radius_decay, X_train):\n",
        "    \n",
        "    self.number_epochs = number_epochs\n",
        "    self.map_size = map_size\n",
        "    self.number_features = number_features\n",
        "    self.learning_rate = initial_learning_rate\n",
        "    self.radius_decay = radius_decay\n",
        "    self.radius = initial_radius\n",
        "    self.initial_learning_rate = initial_learning_rate\n",
        "    \n",
        "    self._initiate_weight(X_train)\n",
        "\n",
        "    self.visualize(X_train, y_train.to_numpy())\n",
        "\n",
        "  def _initiate_weight(self, X_train):\n",
        "\n",
        "    self.weight = []\n",
        "    for i in range(self.map_size[0]):\n",
        "      row_list = []\n",
        "      for j in range(self.map_size[1]):\n",
        "        row_list.append(X_train[np.random.choice(X_train.shape[0])])\n",
        "      self.weight.append(row_list)\n",
        "    self.weight = np.array(self.weight)\n",
        "\n",
        "  def train(self, X_train):\n",
        "    self.weight_changes_list = []\n",
        "    for epoch in range(self.number_epochs):\n",
        "      print(f\"epoch = {epoch}\")\n",
        "      prev_weight = deepcopy(self.weight)\n",
        "      for i, row in enumerate(X_train):\n",
        "        winner = self.find_winner(row)\n",
        "        NS = self.calculate_NS(row, winner)\n",
        "        self.update_weights(NS, row)\n",
        "\n",
        "      self.weight_changes_list.append(np.linalg.norm(prev_weight - self.weight))\n",
        "\n",
        "      self.learning_rate -= (self.initial_learning_rate/self.number_epochs)\n",
        "      self.radius *= self.radius_decay\n",
        "\n",
        "  def find_winner(self, row):\n",
        "    winner_distance = np.inf\n",
        "    winner = (None, None)\n",
        "\n",
        "    for i in range(self.map_size[0]):\n",
        "      for j in range(self.map_size[1]):\n",
        "        neuron_weight = self.weight[i][j]\n",
        "        neuron_distance = self.distance(neuron_weight, row)\n",
        "        \n",
        "        if neuron_distance < winner_distance:\n",
        "          winner_distance = neuron_distance\n",
        "          winner = (i, j)\n",
        "    \n",
        "    return winner\n",
        "\n",
        "  def calculate_NS(self, row, winner):\n",
        "    NS = np.zeros(self.map_size)\n",
        "\n",
        "    for i in range(self.map_size[0]):\n",
        "      for j in range(self.map_size[1]):\n",
        "          neuron_distance = self.distance(np.array((i, j)), np.array(winner))\n",
        "\n",
        "          if neuron_distance <= self.radius:\n",
        "            NS[i][j] = 1 - neuron_distance / self.radius\n",
        "    \n",
        "    return NS\n",
        "\n",
        "  def update_weights(self, NS, row):\n",
        "    for i in range(self.map_size[0]):\n",
        "      for j in range(self.map_size[1]):\n",
        "\n",
        "        delta = row - self.weight[i][j]\n",
        "        self.weight[i][j] += NS[i][j] * self.learning_rate * delta\n",
        "\n",
        "  def visualize(self, X_train, y_train):\n",
        "    number_labels = len(set(y_train))\n",
        "    self.scores = np.zeros((self.map_size[0], self.map_size[1], number_labels))\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "      row = X_train[i]\n",
        "      label = y_train[i]\n",
        "      winner = self.find_winner(row)\n",
        "      self.scores[winner[0]][winner[1]][label] += 1\n",
        "\n",
        "    best_label = np.zeros((self.map_size[0], self.map_size[1]), dtype=int)\n",
        "\n",
        "    for i in range(self.map_size[0]):\n",
        "      for j in range(self.map_size[1]):\n",
        "        if np.all(self.scores[i][j] == 0):\n",
        "          best_label[i][j] == number_labels + 1\n",
        "        else:\n",
        "          best_label[i][j] = np.argmax(self.scores[i][j])\n",
        "\n",
        "    label_colors = self._get_colors(number_labels)\n",
        "    label_colors.append((0, 0, 0))\n",
        "\n",
        "    map_color = []\n",
        "    for i in range(self.map_size[0]):\n",
        "      map_row_color = []\n",
        "      for j in range(self.map_size[1]):\n",
        "        map_row_color.append(label_colors[best_label[i][j]])\n",
        "      map_color.append(map_row_color)\n",
        "    \n",
        "    plt.imshow(map_color)\n",
        "    plt.show()\n",
        "\n",
        "  def plot_weight_changes(self):\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Weight Change\")\n",
        "    plt.title(\"Weight Changing Over Epochs\")\n",
        "    plt.plot(som.weight_changes_list, color=\"purple\")\n",
        "    plt.show()\n",
        "\n",
        "  @staticmethod\n",
        "  def distance(array1, array2):\n",
        "    return np.linalg.norm(array1 - array2)\n",
        "\n",
        "  @staticmethod\n",
        "  def _get_colors(num_colors):\n",
        "      colors=[]\n",
        "      for i in np.arange(0., 360., 360. / num_colors):\n",
        "          hue = i/360.\n",
        "          lightness = (50 + np.random.rand() * 10)/100.\n",
        "          saturation = (90 + np.random.rand() * 10)/100.\n",
        "          colors.append(colorsys.hls_to_rgb(hue, lightness, saturation))\n",
        "      return colors"
      ],
      "metadata": {
        "id": "4xW0sGOGn85x"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "som = SOM(number_epochs=50, map_size=(5,5), number_features=len(X_train.columns),\n",
        "               initial_learning_rate=0.0001, initial_radius=5,\n",
        "               radius_decay=0.95, X_train=X_train.to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "FtrpL06V405j",
        "outputId": "3dfa8389-cf32-4848-c68f-da48942ec710"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAJcElEQVR4nO3d3YtchR3G8efJS6tUqRebC5uEJIiIqY0vLFEbqBAQ4gt6q6BXQm4qRBBEL/0HxBtvgooFRRH0QsQiAWNtNEbXGKMxikGsxkqzW/GNgum6Ty92L1LJZs9Mzpmz8/P7gYWdneXMQ5hvzs7sMuMkAlDHir4HAGgXUQPFEDVQDFEDxRA1UMyqLg66YmIiqzZs6OLQrfvDkSN9TxjM7933goG8//6lfU8oaXb2c83NzZz2ztBJ1Ks2bNCa11/v4tCte3PLlr4nDCT7f933hIFs2jQe94NxMz29bdHr+PEbKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooplHUtnfY/tj2Mdv3dz0KwPCWjNr2SkmPSLpB0mZJt9ve3PUwAMNpcqbeKulYkk+TnJT0jKRbu50FYFhNol4r6YtTLh9f+Nr/sb3T9pTtqbnp6bb2ARhQa0+UJdmdZDLJ5Io1a9o6LIABNYn6S0nrT7m8buFrAJahJlG/Leli25ts/0rSbZJe6HYWgGEt+WL+SWZt3y3pZUkrJT2eZMze1gL45Wj0Dh1JXpL0UsdbALSAvygDiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqCYRi+SMLD3I12UTg7dtj9+/tu+JwzkjUu/7XvCQD75z3jcDyRp1b+u7ntCY9dee2zR6zhTA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxSwZte3HbZ+w/cEoBgE4O03O1E9I2tHxDgAtWTLqJK9J+noEWwC0gMfUQDGtRW17p+0p21NzczNtHRbAgFqLOsnuJJNJJlesmGjrsAAGxI/fQDFNfqX1tKT9ki6xfdz2Xd3PAjCsJd+hI8ntoxgCoB38+A0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFLvkjCMFZv+Vi/O/CnLg7dujcu/bbvCQPZePhw3xMG8smFc31P+MXhTA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxS0Zte73tvbY/tH3E9q5RDAMwnCavUTYr6d4kB22fL+kd23uSfNjxNgBDWPJMneSrJAcXPv9e0lFJa7seBmA4Az2mtr1R0pWSDpzmup22p2xPzU7PtrMOwMAaR237PEnPSbonyXc/vz7J7iSTSSZXrenklYcBNNAoaturNR/0U0me73YSgLPR5NlvS3pM0tEkD3U/CcDZaHKm3ibpTknbbR9a+Lix410AhrTkg98k+yR5BFsAtIC/KAOKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBgnaf2gV626KvvO39f6cbtw3cx1fU8o7Z///XvfE5q7qP0WujI9vU0nTx487YuXcKYGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKWTJq2+fYfsv2e7aP2H5wFMMADGdVg+/5UdL2JD/YXi1pn+2/Jnmz420AhrBk1Jl/EbMfFi6uXvgYnxdzAn5hGj2mtr3S9iFJJyTtSXKg21kAhtUo6iQ/JblC0jpJW21f9vPvsb3T9pTtqZm5mbZ3AmhooGe/k3wjaa+kHae5bneSySSTEysm2toHYEBNnv1eY/uChc/PlXS9pI+6HgZgOE2e/b5Q0l9sr9T8fwLPJnmx21kAhtXk2e/Dkq4cwRYALeAvyoBiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKKbJK58MzJcf1ar9V3dx6Nb9bYIXRu3S6jVb+p7Q3G/6HtDcNf/+fNHrOFMDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQTOOoba+0/a7tF7scBODsDHKm3iXpaFdDALSjUdS210m6SdKj3c4BcLaanqkflnSfpLnFvsH2TttTtqdmZn5qZRyAwS0Zte2bJZ1I8s6Zvi/J7iSTSSYnJla2NhDAYJqcqbdJusX2Z5KekbTd9pOdrgIwtCWjTvJAknVJNkq6TdIrSe7ofBmAofB7aqCYgd52J8mrkl7tZAmAVnCmBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGCdp/6D2tKR/tHzYCUkzLR+zS+O0d5y2SuO1t6utG5KsOd0VnUTdBdtTSSb73tHUOO0dp63SeO3tYys/fgPFEDVQzDhFvbvvAQMap73jtFUar70j3zo2j6kBNDNOZ2oADRA1UMxYRG17h+2PbR+zfX/fe87E9uO2T9j+oO8tS7G93vZe2x/aPmJ7V9+bFmP7HNtv2X5vYeuDfW9qwvZK2+/afnFUt7nso7a9UtIjkm6QtFnS7bY397vqjJ6QtKPvEQ3NSro3yWZJ10j68zL+t/1R0vYkl0u6QtIO29f0vKmJXZKOjvIGl33UkrZKOpbk0yQnNf/Om7f2vGlRSV6T9HXfO5pI8lWSgwuff6/5O9/afledXub9sHBx9cLHsn6W1/Y6STdJenSUtzsOUa+V9MUpl49rmd7xxpntjZKulHSg3yWLW/hR9pCkE5L2JFm2Wxc8LOk+SXOjvNFxiBods32epOck3ZPku773LCbJT0mukLRO0lbbl/W9aTG2b5Z0Isk7o77tcYj6S0nrT7m8buFraIHt1ZoP+qkkz/e9p4kk30jaq+X93MU2SbfY/kzzDxm3235yFDc8DlG/Leli25ts/0rzb3z/Qs+bSrBtSY9JOprkob73nIntNbYvWPj8XEnXS/qo31WLS/JAknVJNmr+PvtKkjtGcdvLPuoks5LulvSy5p/IeTbJkX5XLc7205L2S7rE9nHbd/W96Qy2SbpT82eRQwsfN/Y9ahEXStpr+7Dm/6Pfk2RkvyYaJ/yZKFDMsj9TAxgMUQPFEDVQDFEDxRA1UAxRA8UQNVDM/wBusgU2S2XRNQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "som.train(X_train.to_numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmcywJCYD-xM",
        "outputId": "27504197-6ff6-4ab2-ec33-05ae7cf96b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0\n",
            "epoch = 1\n",
            "epoch = 2\n",
            "epoch = 3\n",
            "epoch = 4\n",
            "epoch = 5\n",
            "epoch = 6\n",
            "epoch = 7\n",
            "epoch = 8\n",
            "epoch = 9\n",
            "epoch = 10\n",
            "epoch = 11\n",
            "epoch = 12\n",
            "epoch = 13\n",
            "epoch = 14\n",
            "epoch = 15\n",
            "epoch = 16\n",
            "epoch = 17\n",
            "epoch = 18\n",
            "epoch = 19\n",
            "epoch = 20\n",
            "epoch = 21\n",
            "epoch = 22\n",
            "epoch = 23\n",
            "epoch = 24\n",
            "epoch = 25\n",
            "epoch = 26\n",
            "epoch = 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "som.visualize(X_train.to_numpy(), y_train.to_numpy())"
      ],
      "metadata": {
        "id": "sUENhn_eEEOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "som.plot_weight_changes()"
      ],
      "metadata": {
        "id": "oIpp9MFdUPhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ntWLEyikrgS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}